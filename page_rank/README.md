#### PageRank定义
> PageRank可以想象是给网页的一个打分，分数越高表示网页的重要性越大。 
> 
首先，可以将Web上的所有网页当成节点，而链接关系可以当成是边，如果A页面中有指向B页面的链接，那么意味着节点A到节点B有一条边，指向B节点，那么这样可以把整个Web想象成一个巨大的有向图。如果一个虚拟用户在节点A，而A的出度为Na，那么下一步虚拟用户在A的任意一个出度节点的概率都是1/Na，因此可以计算出转移矩阵M其中如果j的出度为k，则对于每一个其指向的节点i，Mij=1/k，否则Mij=0

#### 若一个 Markov 过程收敛，那么它的状态转移矩阵AA需要满足:
* stochastic matrix，则行至少存在一个非零值，即必须存在一个外链接（没有外链接的网页被称为dangling pages）；
* 不可约（irreducible），即矩阵AA所对应的有向图GG必须是强连通的，对于任意两个节点u,v∈Vu,v∈V，存在一个从uu到vv的路径；
* 非周期性（aperiodic），即每个节点存在自回路

### Web的结构以及算法的改进
当然，我们抽象出来的图并不一定是强连通的，而且最关键的是，如果一个网页没有出链，那么意味着，通过该算法不断迭代后，能到达该网页的所有节点的概率均为0，因为这些概率最后都转移到了这个页面上去了。或者，存在一组网页集合，他们都有出链，但是只链向该集合中的其他页面，这样虚拟用户只要一踏进这个集合中的某个页面就再也出不去了。同样的，这种情况下能够到达该集合的所有页面概率最后都会变成0，这成为“爬虫陷阱”

所以如果虚拟用户不小心掉进了爬虫陷阱中，就需要有一个机制让它爬出来，但是，有可能作为爬虫本身根本不知道他掉入了这种陷阱中（因为陷阱可能非常大）。所以我们可以增加随机跳转的机制，增加该机制后，迭代公式变成：
$v_n=βMv_{n-1}+(1−β)e/N$

也就是说，有β的概率按照之前的规则跳转，另外1−β的概率，等概率跳到其他的节点。在这里，N表示图中节点的个数，而e表示一个N维的列向量，且每一项值都为1。


### PageRank算法的缺点
这是一个天才的算法，原理简单但效果惊人。然而，PageRank算法还是有一些弊端。

* 第一，没有区分站内导航链接。很多网站的首页都有很多对站内其他页面的链接，称为站内导航链接。这些链接与不同网站之间的链接相比，肯定是后者更能体现PageRank值的传递关系。

* 第二，没有过滤广告链接和功能链接（例如常见的“分享到微博”）。这些链接通常没有什么实际价值，前者链接到广告页面，后者常常链接到某个社交网站首页。

* 第三，对新网页不友好。一个新网页的一般入链相对较少，即使它的内容的质量很高，要成为一个高PR值的页面仍需要很长时间的推广。

* 针对PageRank算法的缺点，有人提出了TrustRank算法。其最初来自于2004年斯坦福大学和雅虎的一项联合研究，用来检测垃圾网站。TrustRank算法的工作原理：先人工去识别高质量的页面(即“种子”页面)，那么由“种子”页面指向的页面也可能是高质量页面，即其TR值也高，与“种子”页面的链接越远，页面的TR值越低。“种子”页面可选出链数较多的网页，也可选PR值较高的网站。

* TrustRank算法给出每个网页的TR值。将PR值与TR值结合起来，可以更准确地判断网页的重要性。

#### PR值计算方法
1. 幂迭代法
2. 特征值法
3. 代数法