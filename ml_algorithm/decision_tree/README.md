## 说明
> C4.5算法是在ID3算法上的一种改进，它与ID3算法最大的区别就是特征选择上有所不同，一个是基于信息增益比，一个是基于信息增益。
之所以这样做是因为信息增益倾向于选择取值比较多的特征(特征越多，条件熵(特征划分后的类别变量的熵)越小，信息增益就越大)；因此在信息增益下面加一个分母，该分母是当前所选特征的熵，注意：这里而不是类别变量的熵了。
这样就构成了新的特征选择准则，叫做信息增益比。为什么加了这样一个分母就会消除ID3算法倾向于选择取值较多的特征呢？
因为特征取值越多，该特征的熵就越大，分母也就越大，所以信息增益比就会减小，而不是像信息增益那样增大了，一定程度消除了算法对特征取值范围的影响。

* ID3: 信息增益
    不能处理连续性特征

    ID3是单变量决策树(在分枝节点上只考虑单个属性)，许多复杂概念的表达困难，属性相互关系强调不够，容易导致决策树中子树的重复或有些属性在决策树的某一路径上被检验多次。
抗噪性差，训练例子中正例和反例的比例较难控制。

    ID3是非递增算法。

    只能处理离散数据。
* C4.5: 信息增益比
    把需要处理的样本(对应根节点)或样本子集(对应子树)按照连续变量的大小从小到大进行排序。

    假设该属性对应的不同的属性值一共有N个，那么总共有N−1个可能的候选分割阈值点，每个候选的分割阈值点的值为上述排序后的属性值中两两前后连续元素的中点，根据这个分割点把原来连续的属性分成bool属性。实际上可以不用检查所有N−1个分割点。(连续属性值比较多的时候，由于需要排序和扫描，会使C4.5的性能有所下降。)

    用信息增益比率选择最佳划分。

* C4.5算法能够处理不完整的数据，常用的处理方法有以下三种：
    给缺失属性赋予最常见的值。

    丢弃含有缺失值的样本。

    根据节点的样例上该属性值出现的情况赋一个概率值。


* CART: Gini指数
    二元切分法
    如果特征值大于给定给值就走左支,否则就走右支.

决策树是一种贪心算法,它要在给定时间内做出最做优的选择,但并不关系
能否到达全局最优