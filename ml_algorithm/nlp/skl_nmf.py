"""
A(mxn) = W(mxk)H(kxn)
此时NMF可以这样解释：我们输入的有m个文本，n个词，而AijAij对应第i个文本的第j个词的特征值，这里最常用的是基于预处理后的标准化TF-IDF值。k是我们假设的主题数，一般要比文本数少。NMF分解后，WikWik对应第i个文本的和第k个主题的概率相关度，而HkjHkj对应第j个词和第k个主题的概率相关度。　　

当然也可以反过来去解释：我们输入的有m个词，n个文本，而AijAij对应第i个词的第j个文本的特征值，这里最常用的是基于预处理后的标准化TF-IDF值。k是我们假设的主题数，一般要比文本数少。NMF分解后，WikWik对应第i个词的和第k个主题的概率相关度，而HkjHkj对应第j个文本和第k个主题的概率相关度。

NMF需要注意的参数有：

　　　　1） n_components：即我们的主题数k, 选择k值需要一些对于要分析文本主题大概的先验知识。可以多选择几组k的值进行NMF，然后对结果人为的进行一些验证。

　　　　2） init : 用于帮我们选择W,HW,H迭代初值的算法， 默认是None,即自动选择值，不使用选择初值的算法。如果我们对收敛速度不满意，才需要关注这个值，从scikit-learn提供的算法中选择一个合适的初值选取算法。

　　　　3）alpha: 即我们第三节中的正则化参数α,需要调参。开始建议选择一个比较小的值，如果发现效果不好在调参增大。

　　　　4) l1_ratio：　即我们第三节中的正则化参数ρρ,L1正则化的比例，仅在α>0α>0时有效，需要调参。开始建议不使用，即用默认值0, 如果对L2的正则化不满意再加上L1正则化。

　　　　从上面可见，使用NMF的关键参数在于主题数的选择n_components和正则化的两个超参数α,ρα,ρ。

　　　　此外，WW矩阵一般在调用fit_transform方法的返回值里获得，而HH矩阵则保存在NMF类的components_成员中。
"""


"""
NMF主题模型小结
	NMF作为一个漂亮的矩阵分解方法，它可以很好的用于主题模型，并且使主题的结果有基于概率分布的解释性。
	但是NMF以及它的变种pLSA虽然可以从概率的角度解释了主题模型，却都只能对训练样本中的文本进行主题识别，
	而对不在样本中的文本是无法识别其主题的。
	根本原因在于NMF与pLSA这类主题模型方法没有考虑主题概率分布的先验知识，
	比如文本中出现体育主题的概率肯定比哲学主题的概率要高，这点来源于我们的先验知识，但是无法告诉NMF主题模型。
	而LDA主题模型则考虑到了这一问题，目前来说，绝大多数的文本主题模型都是使用LDA以及其变体。下一篇我们就来讨论LDA主题模型。
"""

import numpy as np
from sklearn.decomposition import NMF
X = np.array([
	[1,1,5,2,3],
	[0,6,2,1,1],
	[3,4,0,3,1],
	[4,1,5,6,3]
])

model = NMF(n_components=2, alpha=0.01)

W = model.fit_transform(X)
H = model.components_
print(W)
print(H)
"""
[[ 1.67371185  0.02013017]
 [ 0.40564826  2.17004352]
 [ 0.77627836  1.5179425 ]
 [ 2.66991709  0.00940262]]
[[ 1.32014421  0.40901559  2.10322743  1.99087019  1.29852389]
 [ 0.25859086  2.59911791  0.00488947  0.37089193  0.14622829]]

从结果可以看出， 第1,3,4,5个文本和第一个隐含主题更相关，
而第二个文本与第二个隐含主题更加相关。如果需要下一个结论，
我们可以说，第1,3,4,5个文本属于第一个隐含主题，而第二个问题属于第2个隐含主题。
"""