###一.英文文本挖掘预处理流程总结:

    1.英文文本挖掘预处理特点
        a. 不需要健分词,但中文需要分词
        b. 不需要编码,而中文有时候需要编码,转成utf-8
        c. 拼写检查
        d. 词干提取

    2. 英文文本挖掘预处理一: 数据收集
        a. 使用别人做好的语料库和自己用爬虫去在网上去爬自己的语料数据

    3.  英文文本挖掘预处理二：除去数据中非文本部分
        a. 去除html标签,可以用正则,可以用BeautifulSoup,去除非英文字符

    4.  英文文本挖掘预处理三：拼写检查更正
        a. 拼写检查，我们一般用pyenchant类库完成。pyenchant的安装很简单："pip install pyenchant"即可
        ```
        from enchant.checker import SpellChecker
        chkr = SpellChecker("en_US")
        chkr.set_text("Many peope likee to watch In the Name of People.")
        for err in chkr:
        print "ERROR:", err.word
        ```

    5.  英文文本挖掘预处理四：词干提取(stemming)和词形还原(lemmatization)
        在nltk中，做词干提取的方法有PorterStemmer，LancasterStemmer和SnowballStemmer。个人推荐使用SnowballStemmer。这个类可以处理很多种语言，当然，除了中文。
        ```
        from nltk.stem import SnowballStemmer
        stemmer = SnowballStemmer("english") # Choose a language
        stemmer.stem("countries") # Stem a word
        输出是"countri",这个词干并不是一个词。　
        ```　　　

        而如果是做词型还原，则一般可以使用WordNetLemmatizer类，即wordnet词形还原方法。

        from nltk.stem import WordNetLemmatizer
        wnl = WordNetLemmatizer()
        print(wnl.lemmatize('countries'))
        输出是"country",比较符合需求。

        在实际的英文文本挖掘预处理的时候，建议使用基于wordnet的词形还原就可以了。

　　　　  在这里有个词干提取和词型还原的demo，如果是这块的新手可以去看看，上手很合适。

    6. 英文文本挖掘预处理五：转化为小写
       由于英文单词有大小写之分，我们期望统计时像“Home”和“home”是一个词。因此一般需要将所有的词都转化为小写。这个直接用python的API就可以搞定。

    7. 英文文本挖掘预处理六：引入停用词
　　　　在英文文本中有很多无效的词，比如“a”，“to”，一些短词，还有一些标点符号，这些我们不想在文本分析的时候引入，因此需要去掉，这些词就是停用词。个人常用的英文停用词表下载地址在这。当然也有其他版本的停用词表，不过这个版本是我常用的。

　　　　在我们用scikit-learn做特征处理的时候，可以通过参数stop_words来引入一个数组作为停用词表。这个方法和前文讲中文停用词的方法相同，这里就不写出代码，大家参考前文即可。

    8. 英文文本挖掘预处理七：特征处理
　　　　现在我们就可以用scikit-learn来对我们的文本特征进行处理了，在文本挖掘预处理之向量化与Hash Trick中，我们讲到了两种特征处理的方法，向量化与Hash Trick。而向量化是最常用的方法，因为它可以接着进行TF-IDF的特征处理。在文本挖掘预处理之TF-IDF中，我们也讲到了TF-IDF特征处理的方法。

　　　　TfidfVectorizer类可以帮助我们完成向量化，TF-IDF和标准化三步。当然，还可以帮我们处理停用词。这部分工作和中文的特征处理也是完全相同的，大家参考前文即可。

    9. 英文文本挖掘预处理八：建立分析模型
　　　　有了每段文本的TF-IDF的特征向量，我们就可以利用这些数据建立分类模型，或者聚类模型了，或者进行主题模型的分析。此时的分类聚类模型和之前讲的非自然语言处理的数据分析没有什么两样。因此对应的算法都可以直接使用。而主题模型是自然语言处理比较特殊的一块。


### 词袋模型
    总结下词袋模型的三部曲：分词（tokenizing），统计修订词特征值（counting）与标准化（normalizing）。

### Hash Trick
    在大规模的文本处理中，由于特征的维度对应分词词汇表的大小，所以维度可能非常恐怖，此时需要进行降维，不能直接用我们上一节的向量化方法。而最常用的文本降维方法是Hash Trick。说到Hash，一点也不神秘，学过数据结构的同学都知道。这里的Hash意义也类似。

    在Hash Trick里，我们会定义一个特征Hash后对应的哈希表的大小，这个哈希表的维度会远远小于我们的词汇表的特征维度，因此可以看成是降维


    一般来说，只要词汇表的特征不至于太大，大到内存不够用，肯定是使用一般意义的向量化比较好。因为向量化的方法解释性很强，我们知道每一维特征对应哪一个词，进而我们还可以使用TF-IDF对各个词特征的权重修改，进一步完善特征的表示。

    而Hash Trick用大规模机器学习上，此时我们的词汇量极大，使用向量化方法内存不够用，而使用Hash Trick降维速度很快，降维后的特征仍然可以帮我们完成后续的分类和聚类工作。当然由于分布式计算框架的存在，其实一般我们不会出现内存不够的情况。因此，实际工作中我使用的都是特征向量化。


### TF-IDF小结
　　TF-IDF是非常常用的文本挖掘预处理基本步骤，但是如果预处理中使用了Hash Trick，则一般就无法使用TF-IDF了，因为Hash Trick后我们已经无法得到哈希后的各特征的IDF的值。使用了IF-IDF并标准化以后，我们就可以使用各个文本的词特征向量作为文本的特征，进行分类或者聚类分析。

　　当然TF-IDF不光可以用于文本挖掘，在信息检索等很多领域都有使用。因此值得好好的理解这个方法的思想。



### LSI主题模型总结
    LSI是最早出现的主题模型了，它的算法原理很简单，一次奇异值分解就可以得到主题模型，同时解决词义的问题，非常漂亮。但是LSI有很多不足，导致它在当前实际的主题模型中已基本不再使用。

    主要的问题有：

    1） SVD计算非常的耗时，尤其是我们的文本处理，词和文本数都是非常大的，对于这样的高维度矩阵做奇异值分解是非常难的。

    2） 主题值的选取对结果的影响非常大，很难选择合适的k值。

    3） LSI得到的不是一个概率模型，缺乏统计基础，结果难以直观的解释。

    对于问题1），主题模型非负矩阵分解（NMF）可以解决矩阵分解的速度问题。对于问题2），这是老大难了，大部分主题模型的主题的个数选取一般都是凭经验的，较新的层次狄利克雷过程（HDP）可以自动选择主题个数。对于问题3），牛人们整出了pLSI(也叫pLSA)和隐含狄利克雷分布(LDA)这类基于概率分布的主题模型来替代基于矩阵分解的主题模型。