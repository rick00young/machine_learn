## AdaBoost

AdaBoost是adaptive boosting（自适应boosting）的缩写，
运行过程如下：赋予训练集中的每个样本一个权值D，一开始权值都是相等的，
然后训练一个弱分类器并计算错误率，分类正确的样本会降低权重，
而分类错误的样本会提高权重，接着再次根据权重训练一个弱分类器，
这么做是为了让新的弱分类器在训练中更加关注未被分类正确的样本。
为了综合所有弱分类器的结果，每个分类器都有一个权重α，基于错误率ε计算的。

错误率定义为:

$$ε = \frac{未正确分类的样本数据}{所有样本数}$$

而α的计算公式为:

$$α=\frac{1}{2}ln\Biggl(\frac{1-ε}{ε}\Biggr)$$

如果某个样本被正确分类,那么该样本的权重更改为:

$$D_i^{(t+1)}=\frac{{D_i^{(t)}}e^{-α}}{Sum(D)}$$

而如果某个样本被错误分类,该样本权重更改为:

$$D_i^{(t+1)}=\frac{D_i^{(t+1)}e^{α}}{Sum(D)}$$


单层决策树弱分类器
单层决策树(decision stump)也叫决策树桩，是一种简单的决策树，仅基于单个特征做决策。

伪代码

    将最小错误率minError设为+∞
    对数据集中的每一个特征（第一层循环）：
        对每个步长（第二层循环）：
            对每个不等号（第三层循环）：
                建立一棵单层决策树并利用加权数据集对它进行测试
                如果错误率低于minError，则将当前单层决策树设为最佳单层决策树
    返回最佳单层决策树


这里对Adaboost算法的优缺点做一个总结。

Adaboost的主要优点有：

* 1）Adaboost作为分类器时，分类精度很高

* 2）在Adaboost的框架下，可以使用各种回归分类模型来构建弱学习器，非常灵活。

* 3）作为简单的二元分类器时，构造简单，结果可理解。

* 4）不容易发生过拟合

Adaboost的主要缺点有：

* 1）对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。

