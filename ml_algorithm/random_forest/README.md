## 随机森林

[https://www.cnblogs.com/zdz8207/p/DeepLearning-rfa-python.html](https://www.cnblogs.com/zdz8207/p/DeepLearning-rfa-python.html)

目前的集成学习方法大致可分为两大类，
即个体学习器之间存在强依赖关系，必须串行生成的序列化方法，
以及个体学习器间不存在强依赖关系，可同时生成的并行化方法；

前者的代表是Boosting，后者的代表是Bagging和“随机森林”（Random Forest）

在对预测输出进行结合时，Bagging通常对分类任务使用简单投票法，对回归任务使用简单平均法

随机森林是Bagging的一个扩展。随机森林在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择（即引入随机特征选择）。传统决策树在选择划分属性时时在当前节点的属性集合（假定有d个属性）中选择一个最优属性；而在随机森林中，对基决策树的每个节点，先从该节点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分。这里的参数k控制了随机性的引入程度：若令k=d,则基决策树的构建与传统决策树相同；若令k=1，则是随机选择一个属性进行划分。


简单来说，随机森林就是对决策树的集成，但有两点不同：

* （1）采样的差异性：从含m个样本的数据集中有放回的采样，得到含m个样本的采样集，用于训练。这样能保证每个决策树的训练样本不完全一样。

* （2）特征选取的差异性：每个决策树的n个分类特征是在所有特征中随机选择的（n是一个需要我们自己调整的参数）

随机森林需要调整的参数有：

* （1）    决策树的个数

* （2）    特征属性的个数

* （3）    递归次数（即决策树的深度）

