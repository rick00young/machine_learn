## 感知机
[http://www.cnblogs.com/pinard/p/6042320.html](http://www.cnblogs.com/pinard/p/6042320.html)
### 1. 感知机模型
因为研究透了感知机模型，学习支持向量机的话会降低不少难度。同时如果研究透了感知机模型，再学习神经网络，深度学习，也是一个很好的起点.

使用感知机一个最大的前提，就是数据是线性可分的。这严重限制了感知机的使用场景。它的分类竞争对手在面对不可分的情况时，比如支持向量机可以通过核技巧来让数据在高维可分，神经网络可以通过激活函数和增加隐藏层来让数据可分。

用数学语言来说,每个样本对于n维特征和一个二元类别输出:
$$(x_1^{(0)}, x_2^{(0)}, ...,x_n^{(0)}, y_0), (x_1^{(1)}, x_2^{(1)}, ...,x_n^{(1)}, y_1), ..., (x_1^{(m)}, x_2^{(m)}, ...,x_n^{(m)}, y_m)$$

目标是找到一个超平面,即:
$$\theta_0+\theta_1 x_1+...+\theta_n x_n=0$$

让其中一种类别的样本都满足$\theta_0+\theta_1 x_1+...+\theta_n x_n>0$ ，让另一种类别的样本都满足$\theta_0+\theta_1 x_1+...+\theta_n x_n<0$  ，从而得到线性可分。如果数据线性可分，这样的超平面一般都不是唯一的，也就是说感知机模型可以有多个解。

为了简化这个超平面的写法，我们增加一个特征$x_0=1$ ，这样超平面为$\sum_{i=0}^{n}{\theta_ix_i}=0$。进一步用向量来表示为： $\theta\cdot x=0$,其中$\theta$为(n+1)x1的向量，x为(n+1)x1的向量, $\cdot$为内积，后面我们都用向量来表示超平面。

而感知机的模型可以定义为:$y=sign(\theta \cdot x)$, 其中:
$$sign(x)=
\begin{cases}
-1, & {x<0} \\
1, & {x\ge0}
\end{cases}
$$

### 2. 感知机模型损失函数
令$\theta\cdot x>0$的样本类别输出值为1, $\theta\cdot x<0$的样本类别输出值为-1.
因为正确分类的样本满足 $\theta\cdot x>0$，而错误分类的样本满足 $\theta\cdot x<0$。我们损失函数的优化目标，就是期望使误分类的所有样本，到超平面的距离之和最小。

由于$\theta\cdot x<0$, 所以每一个误分类的样本$i$,到超平面的距离是:
$$\left.{-y^{(i)}\theta\cdot x^{(i)}}\middle/{||\theta||_2}\right.$$

其中$||\theta||_2$是L2范数.

我们假设所有误分类的点的集合为M，则所有误分类的样本到超平面的距离之和为：
$$\left.{-\sum_{x_i\in M}y^{(i)}\theta\cdot x^{(i)}}\middle/||\theta||_2\right.$$
以上就是感知机模型的损失函数.

我们研究可以发现，分子和分母都含有θ,当分子的θ扩大N倍时，分母的L2范数也会扩大N倍。也就是说，分子和分母有固定的倍数关系。那么我们可以固定分子或者分母为1，然后求另一个即分子自己或者分母的倒数的最小化作为损失函数，这样可以简化我们的损失函数。在感知机模型中，我们采用的是保留分子，即最终感知机模型的损失函数简化为：
$$J(\theta)=-\sum_{x_i\in M}y^{(i)}\theta\cdot x^{(i)}$$

如果了解过支持向量机，就发现支持向量机采用的是固定分子为1，然后求$1/||θ||_2$的最小化。采用不同的损失函数主要与它的后面的优化算法有关系

### 3. 感知机模型损失函数的优化
感知机模型选择的是采用随机梯度下降，这意味着我们每次仅仅需要使用一个误分类的点来更新梯度。

损失函数基于θ向量的的偏导数为：
$$\frac{\partial}{\partial\theta}J(\theta)=-\sum_{x_i\in M}y^{(i)}x^{(i)}$$

$\theta$梯度下降迭代公式为:
$$\theta=\theta+\alpha\sum_{x_i\in M}y^{(i)}x^{(i)}$$

由于我们采用随机梯度下降，所以每次仅仅采用一个误分类的样本来计算梯度，假设采用第i个样本来更新梯度，则简化后的θθ向量的梯度下降迭代公式为：
$$\theta=\theta+\alpha y^{(i)}x^{(i)}$$
其中$\alpha$为步长,$y^{(i)}$为样本输出1或-1, $x^{(i)}$为(n+1)x1的向量.

### 感知机模型的算法对偶形式
感知机模型的算法原始形式$\theta=\theta+\alpha y^{(i)}x^{(i)}$可以看出，我们每次梯度的迭代都是选择的一个样本来更新θ向量。最终经过若干次的迭代得到最终的结果。对于从来都没有误分类过的样本，他被选择参与θ迭代的次数是0，对于被多次误分类而更新的样本j，它参与θ迭代的次数我们设置为$m_j$。如果令θ向量初始值为0向量， 这样我们的θ向量的表达式可以写为：
$$\theta=\alpha\sum_{j=1}^m{m_j y^{(i)}x^{(i)}}$$
其中$m_j$为样本$(x^{(i)}, y^{(i)})$在随机梯度下降到当前这一步之前因误分类面更新的次数.

每一个样本$(x^{(i)}, y^{(i)})$的$m_j$的初始值为0，每当此样本在某一次梯度下降迭代中因误分类而更新时，$m_j$的值加1。

由于步长α为常量，我们令$\beta_j=αm_j$,这样θ向量的表达式为:
$$\theta=\sum_{j=1}^m{\beta_j y^{(i)}x^{(i)}}$$

在每一步判断误分类条件的地方，我们用 $y^{(i)}\theta\cdot x^{(i)}<0$ 的变种 $y^{(i)}\sum_{j=1}^m{\beta_j y^{(j)}x^{(j)}\cdot x^{(i)}}<0$ 来判断误分类。注意到这个判断误分类的形式里面是计算两个样本$x^{(i)}$和$x^{(j)}$的内积，而且这个内积计算的结果在下面的迭代次数中可以重用。如果我们事先用矩阵运算计算出所有的样本之间的内积，那么在算法运行时， 仅仅一次的矩阵内积运算比多次的循环计算省时。 计算量最大的判断误分类这儿就省下了很多的时间，，这也是对偶形式的感知机模型比原始形式优的原因。

样本的内积矩阵称为Gram矩阵，它是一个对称矩阵，记为
$$G=[x^{(i)}\cdot x^{(j)}]$$