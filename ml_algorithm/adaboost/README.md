## AdaBoost

AdaBoost是adaptive boosting（自适应boosting）的缩写，
运行过程如下：赋予训练集中的每个样本一个权值D，一开始权值都是相等的，
然后训练一个弱分类器并计算错误率，分类正确的样本会降低权重，
而分类错误的样本会提高权重，接着再次根据权重训练一个弱分类器，
这么做是为了让新的弱分类器在训练中更加关注未被分类正确的样本。
为了综合所有弱分类器的结果，每个分类器都有一个权重α，基于错误率ε计算的。

错误率定义为:

$$ε = \frac{未正确分类的样本数据}{所有样本数}$$

而α的计算公式为:

$$α=\frac{1}{2}ln\Biggl(\frac{1-ε}{ε}\Biggr)$$

如果某个样本被正确分类,那么该样本的权重更改为:

$$D_i^{(t+1)}=\frac{{D_i^{(t)}}e^{-α}}{Sum(D)}$$

而如果某个样本被错误分类,该样本权重更改为:

$$D_i^{(t+1)}=\frac{D_i^{(t+1)}e^{α}}{Sum(D)}$$


单层决策树弱分类器
单层决策树(decision stump)也叫决策树桩，是一种简单的决策树，仅基于单个特征做决策。

伪代码

    将最小错误率minError设为+∞
    对数据集中的每一个特征（第一层循环）：
        对每个步长（第二层循环）：
            对每个不等号（第三层循环）：
                建立一棵单层决策树并利用加权数据集对它进行测试
                如果错误率低于minError，则将当前单层决策树设为最佳单层决策树
    返回最佳单层决策树


