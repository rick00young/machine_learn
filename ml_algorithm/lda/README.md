# 文本主题模型之潜在语义索引(LSI)

svd分解

如果把上式用到我们的主题模型，则SVD可以这样解释：我们输入的有m个文本，每个文本有n个词。而Aij则对应第i个文本的第j个词的特征值，这里最常用的是基于预处理后的标准化TF-IDF值。k是我们假设的主题数，一般要比文本数少。SVD分解后，Uil对应第i个文本和第l个主题的相关度。Vjm对应第j个词和第m个词义的相关度。Σlm对应第l个主题和第m个词义的相关度。


主要的问题有：

　　　　1） SVD计算非常的耗时，尤其是我们的文本处理，词和文本数都是非常大的，对于这样的高维度矩阵做奇异值分解是非常难的。

　　　　2） 主题值的选取对结果的影响非常大，很难选择合适的k值。

　　　　3） LSI得到的不是一个概率模型，缺乏统计基础，结果难以直观的解释。

　　　　对于问题1），主题模型非负矩阵分解（NMF）可以解决矩阵分解的速度问题。对于问题2），这是老大难了，大部分主题模型的主题的个数选取一般都是凭经验的，较新的层次狄利克雷过程（HDP）可以自动选择主题个数。对于问题3），牛人们整出了pLSI(也叫pLSA)和隐含狄利克雷分布(LDA)这类基于概率分布的主题模型来替代基于矩阵分解的主题模型。

# 文本主题模型之非负矩阵分解(NMF)
## 2. NMF的优化思路
NMF期望找到这样的两个矩阵W,H，使WH的矩阵乘积得到的矩阵对应的每个位置的值和原矩阵A对应位置的值相比误差尽可能的小。用数学的语言表示就是：
$$\underbrace{arg\space min}_{W,H}=\frac{1}{2}\sum_{i,j}{(A_{ij}-(WH)_{ij})^2}$$
用矩阵表示为:
$$\underbrace{arg\space min}_{W,H}=\frac{1}{2}\sum_{i,j}{||A_{ij}-(WH)_{ij}||_{Fro}^2}$$

其中，||∗||Fro为Frobenius范数。

此时NMF可以这样解释：我们输入的有m个文本，n个词，而Aij对应第i个文本的第j个词的特征值，这里最常用的是基于预处理后的标准化TF-IDF值。k是我们假设的主题数，一般要比文本数少。NMF分解后，WikWik对应第i个文本的和第k个主题的概率相关度，而HkjHkj对应第j个词和第k个主题的概率相关度。

## 6. NMF主题模型小结
　　　　NMF作为一个漂亮的矩阵分解方法，它可以很好的用于主题模型，并且使主题的结果有基于概率分布的解释性。但是NMF以及它的变种pLSA虽然可以从概率的角度解释了主题模型，却都只能对训练样本中的文本进行主题识别，而对不在样本中的文本是无法识别其主题的。根本原因在于NMF与pLSA这类主题模型方法没有考虑主题概率分布的先验知识，比如文本中出现体育主题的概率肯定比哲学主题的概率要高，这点来源于我们的先验知识，但是无法告诉NMF主题模型。而LDA主题模型则考虑到了这一问题，目前来说，绝大多数的文本主题模型都是使用LDA以及其变体。下一篇我们就来讨论LDA主题模型。